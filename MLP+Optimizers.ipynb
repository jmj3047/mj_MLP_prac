{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer Perceptron(MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Library & requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "drive_project_root = \"/home/jmj3047/mj_MLP_prac\"\n",
    "sys.path.append(drive_project_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch_optimizer import RAdam\n",
    "from torch_optimizer import AdamP\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pwd\n",
    "data_root = os.path.join(os.getcwd(), 'data')\n",
    "\n",
    "#preprocessing & 데이터 셋 정의\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5],[0.5]) #mean, std\n",
    "    ]\n",
    ")\n",
    "\n",
    "fashion_mnist_dataset = FashionMNIST(data_root, download=True, train=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "from data_utils import dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets= dataset_split(fashion_mnist_dataset, split=[0.9,0.1])\n",
    "\n",
    "train_dataset = datasets['train']\n",
    "val_dataset = datasets['val']\n",
    "\n",
    "train_batch_size = 100\n",
    "val_batch_size = 10\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size = train_batch_size, shuffle=True, num_workers=1\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size = val_batch_size, shuffle=True, num_workers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "for sample_batch in train_dataloader:\n",
    "    print(sample_batch[0].shape, sample_batch[1].shape)\n",
    "    break\n",
    "\n",
    "#torch.Size([100, 1, 28, 28]) batchsize, channel, width, height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델(Multi-Layer Perceptron) (MLP) 정의\n",
    "## 모델 MLPWithDropout 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch. nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Model\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim=int, h1_dim = int, h2_dim = int, out_dim = int):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(in_dim, h1_dim)\n",
    "        self.linear2 = nn.Linear(h1_dim, h2_dim)\n",
    "        self.linear3 = nn.Linear(h2_dim, out_dim)\n",
    "        self.relu = F.relu #activation 함수 정의\n",
    "\n",
    "        pass\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = torch.flatten(input, start_dim=1)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        out = self.linear3(x)\n",
    "        out = F.sigmoid(out) #binary classification은 softmax로 사용\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class MLPWithDropout(MLP):\n",
    "    def __init__(self, in_dim: int, h1_dim: int, h2_dim: int, out_dim: int, dropout_prob: float):\n",
    "        super().__init__(in_dim, h1_dim, h2_dim, out_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = torch.flatten(input, start_dim=1)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.dropout2(x)\n",
    "        out = self.linear3(x)\n",
    "        # out = F.softmax(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 선언 및 손실함수, 최적화(Optimizer) 정의, Tensorboard Logger 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPWithDropout\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.optim' has no attribute 'AdamP'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b3aeb593d388>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# optimizer = torch.optim.SGD(model.parameters(), lr=lr)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0moptimizer_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.optim' has no attribute 'AdamP'"
     ]
    }
   ],
   "source": [
    "#define model\n",
    "# model = MLP(28*28, 128, 64, 10)\n",
    "model = MLPWithDropout(28*28, 128,64,10, dropout_prob=0.3)\n",
    "model_name = type(model).__name__\n",
    "print(model_name)\n",
    "\n",
    "#define loss\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "#define optimizer\n",
    "lr=1e-3\n",
    "# optimizer = torch.optim.RAdam(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "optimizer = torch.optim.AdamP(model.parameters(), lr=lr)\n",
    "optimizer_name = type(optimizer).__name__\n",
    "\n",
    "\n",
    "#define scheduler\n",
    "scheduler = None\n",
    "scheduler_name = type(scheduler).__name__ if scheduler is not None else \"no\"\n",
    "\n",
    "max_epoch = 10\n",
    "\n",
    "#define tensorboard logger\n",
    "run_name = f\"{datetime.now().isoformat(timespec='seconds')}-{model_name}-{optimizer_name}_optim_{lr}_lr_with_{scheduler_name}_scheduler\"\n",
    "log_dir = f\"runs/{run_name}\"\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "# writer = SummaryWriter()\n",
    "log_interval =100\n",
    "\n",
    "#define wandb\n",
    "project_name='fastcapmus_fashion_mnist_tutorials'\n",
    "run_tags = [project_name]\n",
    "wandb.init(\n",
    "    project=project_name,\n",
    "    name=run_name,\n",
    "    tags=run_tags,\n",
    "    config={\"lr\":lr, \"model_name\":model_name, \"optimizer_name\":optimizer_name, \"scheduler_name\": scheduler_name},\n",
    "    reinit=True\n",
    ")\n",
    "\n",
    "# set save model path\n",
    "log_model_path = os.path.join(log_dir, \"models\")\n",
    "os.makedirs(log_model_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping callback Object Class 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With some modifications, source is from https://github.com/Bjarten/early-stopping-pytorch\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.ckpt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.ckpt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None: #loss가 최소화 됐을때 save_checkpoint를 저장함\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta: #모델 성능이 더이상 개선되지 않는다 했을 때 early stopping이 됨\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience: #개선이 안됐다고 바로 하는게 아니라 조금 기다렸다가 stop함\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        \n",
    "        filename = self.path.split('/')[-1]\n",
    "        save_dir = os.path.dirname(self.path)\n",
    "        torch.save(model, os.path.join(save_dir, f\"val_loss-{val_loss}-{filename}\")) #어떤 score에서 멈췄는지가 중요하기 때문에 그걸 print해줌\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs/ #여기다가 log를 쌓을거고 거기 있는걸 plot해달라는 의미\n",
    "\n",
    "#define EarlyStopping\n",
    "early_stopper = EarlyStopping(\n",
    "    patience=3, verbose=True, path = os.path.join(log_model_path, \"model.ckpt\")\n",
    ")\n",
    "\n",
    "#do train with validation\n",
    "train_step = 0\n",
    "for epoch in range(1, max_epoch+1):\n",
    "    #validation step\n",
    "    with torch.no_grad(): # optimizer가 업데이트 하면 안됨\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "        model.eval()\n",
    "\n",
    "        for val_batch_idx, (val_images, val_labels) in enumerate(\n",
    "            tqdm(val_dataloader, position=0, leave=True, desc = 'validation')\n",
    "        ):\n",
    "            #forward\n",
    "            val_outputs = model(val_images)\n",
    "            _, val_preds = torch.max(val_outputs, 1)\n",
    "            \n",
    "            #loss & acc\n",
    "            val_loss += loss_function(val_outputs, val_labels) / val_outputs.shape[0] #이게 batch size. batch size만큼 평균을 내겠다는 뜻\n",
    "            val_corrects += torch.sum(val_preds == val_labels.data) / val_outputs.shape[0]\n",
    "        \n",
    "        #valid step logging\n",
    "        val_epoch_loss = val_loss / len(val_dataloader)\n",
    "        val_epoch_acc = val_corrects/len(val_dataloader)\n",
    "        print(\n",
    "            f\"{epoch} epoch, {train_step} step: val_loss: {val_epoch_loss}, val_acc: {val_epoch_acc}\" \n",
    "        )\n",
    "\n",
    "        #tensorboard log\n",
    "        writer.add_scalar(\"Loss/val\", val_epoch_loss, train_step)\n",
    "        writer.add_scalar(\"Acc/val\", val_epoch_acc, train_step)\n",
    "        writer.add_images(\"Images/val\", val_images, train_step)\n",
    "\n",
    "        #wandb log\n",
    "        wandb.log({\n",
    "            \"Loss/val\":val_epoch_loss,\n",
    "            \"Acc/val\": val_epoch_acc,\n",
    "            \"Images/val\": wandb.Image(val_images),\n",
    "            \"Outputs/val\": wandb.Histogram(val_outputs.detach().numpy()),\n",
    "            \"Preds/val\": wandb.Histogram(val_preds.detach().numpy()),\n",
    "            \"Labels/val\": wandb.Histogram(val_labels.data.detach().numpy()),\n",
    "        }, step=train_step)\n",
    "\n",
    "        # check early stopping point & save model if model reached the best performance\n",
    "        early_stopper(val_epoch_loss, model)\n",
    "        if early_stopper.early_stop:\n",
    "            break\n",
    "        \n",
    "        #train step\n",
    "        current_loss = 0\n",
    "        current_corrects = 0\n",
    "        model\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "    #train step\n",
    "    for batch_idx, (images, labels) in enumerate(\n",
    "         tqdm(train_dataloader, position=0, leave=True, desc = 'train')\n",
    "    ):\n",
    "        current_loss = 0.0\n",
    "        current_corrects = 0\n",
    "\n",
    "        #get predictions\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        # print(outputs)\n",
    "        # print(preds)\n",
    "        \n",
    "        #get loss\n",
    "        loss = loss_function(outputs, labels)\n",
    "\n",
    "\n",
    "        ###### 여기까지가 forward ###\n",
    "\n",
    "        #Backpropagation\n",
    "\n",
    "        #optimitizer 초기화(zero화)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        #perfrom optimization\n",
    "        optimizer.step()\n",
    "\n",
    "        current_loss +=loss.item()\n",
    "        current_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        if train_step % log_interval == 0:\n",
    "            train_loss = current_loss / log_interval\n",
    "            train_acc = current_corrects/log_interval\n",
    "            print(\n",
    "                f\"{train_step}: train_loss: {train_loss}, train_acc: {train_acc}\" \n",
    "            )\n",
    "\n",
    "            #tensorboard log\n",
    "            writer.add_scalar(\"Loss/train\", train_step)\n",
    "            writer.add_scalar(\"Acc/train\", train_step)\n",
    "            writer.add_images(\"Images/train\", images, train_step)\n",
    "            writer.add_graph(model, images)\n",
    "            \n",
    "            # wandb log\n",
    "            wandb.log({\n",
    "                \"Loss/train\": train_loss,\n",
    "                \"Acc/train\": train_acc,\n",
    "                \"Images/train\": wandb.Image(images),\n",
    "                \"Outputs/train\": wandb.Histogram(outputs.detach().cpu().numpy()),\n",
    "                \"Preds/train\": wandb.Histogram(preds.detach().cpu().numpy()),\n",
    "                \"Labels/train\": wandb.Histogram(labels.data.detach().cpu().numpy()),\n",
    "            }, step=train_step)\n",
    "\n",
    "            current_loss = 0\n",
    "            current_corrects = 0\n",
    "\n",
    "        train_step += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "# os.makedirs(\"./logs/models\", exist_ok=True)\n",
    "# torch.save(model, os.path.join(log_model_path, \"model.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "loaded_model = torch.load(os.path.join(log_model_path, \"val_loss-0.03366972133517265-model.ckpt\"))\n",
    "loaded_model.eval()\n",
    "print(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=0):\n",
    "    \"numpy softmax\"\n",
    "    max = np.max(x, axis=axis, keepdims=True)\n",
    "    e_x = np.exp(x - max)\n",
    "    sum = np.sum(e_x, axis = axis, keepdims=True)\n",
    "    f_x = e_x / sum\n",
    "    return f_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_size = 100\n",
    "test_dataset = FashionMNIST(data_root, download=True, train=False, transform=transforms.ToTensor())\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=1)\n",
    "\n",
    "test_labels_list = []\n",
    "test_preds_list = []\n",
    "test_outputs_list = []\n",
    "\n",
    "\n",
    "for i, (test_images, test_labels) in enumerate(tqdm(test_dataloader, position=0, leave=True, desc=\"testing\")):\n",
    "    #forward\n",
    "    test_outputs = loaded_model(test_images)\n",
    "    _, test_preds = torch.max(test_outputs, 1)\n",
    "\n",
    "    final_outs = softmax(test_outputs.detach().numpy(), axis=1)\n",
    "    test_outputs_list.extend(final_outs)\n",
    "    test_preds_list.extend(test_preds.detach().numpy())\n",
    "    test_labels_list.extend(test_preds.detach().numpy())\n",
    "\n",
    "test_preds_list = np.array(test_preds_list)\n",
    "test_labels_list = np.array(test_labels_list)\n",
    "\n",
    "print(f\"acc: {np.mean(test_preds_list==test_labels_list)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "fpr={}\n",
    "tpr={}\n",
    "thresh={}\n",
    "n_class = 10\n",
    "\n",
    "for i in range(n_class):\n",
    "    fpr[i], tpr[i], thresh[i] = roc_curve(test_labels_list, np.array(test_outputs_list)[:,i], pos_label=i)\n",
    "\n",
    "#print(fpr)\n",
    "\n",
    "#plot\n",
    "for i in range(n_class):\n",
    "    plt.plot(fpr[i], tpr[i], linestyle = '--', label=f\"class{i} vs Rest\")\n",
    "plt.title(\"Multi-class ROC Curve\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc = \"best\")\n",
    "plt.show()\n",
    "\n",
    "print(roc_auc_score(test_labels_list, test_outputs_list, multi_class='ovo', average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7244abc13d988a7e7d2c4c53272249ae82a63923609912adda01122398923101"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
